---
title: "SR model Diagnostics"
author: "Authors"
date: "2025-03-05"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
library(here)
library(tidyverse)
library(ggplotify) #for as.ggplot() to help mcmc_combo() plotting
library(rstan)
library(bayesplot)
library(kableExtra)
source(here("analysis/R/data_functions.R"))

# read in model fits ---------------------------------------------------------------------
AR1.fits <- lapply(list.files(here("analysis/data/generated/model_fits/AR1"),
                              full.names = T), 
                   readRDS)
names(AR1.fits) <- unique(sp_har$CU)[order(unique(sp_har$CU))]

TVA.fits <- lapply(list.files(here("analysis/data/generated/model_fits/TVA"), 
                              full.names = T), 
                   readRDS)
names(TVA.fits) <- unique(sp_har$CU)[order(unique(sp_har$CU))]
```

Common diagnostic plots for spawner-recruit models by CU.  

```{r diagnostics, results = 'asis'}
# describe diagnostics in loop -----------------------------------------------------------
min_ESS_AR1 <- NULL
max_Rhat_AR1 <- NULL
min_ESS_TVA <- NULL
max_Rhat_TVA <- NULL

for(i in unique(sp_har$CU)){
  sub_dat <- filter(sp_har, CU==i)
  cat('\n')
  cat("## ", i, "\n") #escape loop and print CU name
  #diagnostics associated w/ AR1 model---
  sub_AR1_summary <- as.data.frame(rstan::summary(AR1.fits[[i]])$summary)
  
  sub_AR1_pars <- rstan::extract(AR1.fits[[i]])
  
  min_ESS_AR1 <- rbind(min_ESS_AR1, data.frame(CU=i, 
                                       ESS=round(min(sub_AR1_summary$n_eff, na.rm=T))))
  
  max_Rhat_AR1 <- rbind(max_Rhat_AR1, data.frame(CU=i, 
                                         Rhat=round(max(sub_AR1_summary$Rhat, na.rm=T), 3)))
  
  R <- (sub_dat$harv+sub_dat$spwn)
  R_rep <- sub_AR1_pars$H_rep[1:500,] + sub_AR1_pars$S_rep[1:500,]
  
  ppc_dens_overlay(R, R_rep) +
    xlim(NA, quantile(R_rep, 0.99)) +
    theme(legend.position = "none") +
    labs(y = "density", x = "y_est", title = paste(i, "posterior predictive check"))
  
  p <- mcmc_combo(AR1.fits[[i]], pars = c("beta", "lnalpha", "sigma_R", "phi"), #for AR1
             combo = c("dens_overlay", "trace"),
             gg_theme = legend_none()) |> 
    as.ggplot() +
    labs(title = paste(i, "leading parameters"))
  print(p)

  p <- mcmc_combo(AR1.fits[[i]], pars = c("D_scale", "D_sum"),
             combo = c("dens_overlay", "trace"),
             gg_theme = legend_none())|> 
    as.ggplot() +
    labs(title = paste(i, "age pars"))
    print(p)

  p <- mcmc_combo(AR1.fits[[i]], pars = c("Dir_alpha[1]", "Dir_alpha[2]", 
                               "Dir_alpha[3]", "Dir_alpha[4]"),
             combo = c("dens_overlay", "trace"),
             gg_theme = legend_none())|> 
    as.ggplot() +
    labs(title = paste(i, "age probs"))
    print(p)

  #diagnostics associated w/ TVA model---
  sub_TVA_summary <- as.data.frame(rstan::summary(TVA.fits[[i]])$summary)
  
  sub_TVA_pars <- rstan::extract(TVA.fits[[i]])
  
  min_ESS_TVA <- rbind(min_ESS_TVA, data.frame(CU=i, 
                                               ESS=round(min(sub_TVA_summary$n_eff, na.rm=T))))
  
  max_Rhat_TVA <- rbind(max_Rhat_TVA, data.frame(CU=i, 
                                                 Rhat=round(max(sub_TVA_summary$Rhat, na.rm=T), 3)))
  
  R <- (sub_dat$harv+sub_dat$spwn)
  R_rep <- sub_TVA_pars$H_rep[1:500,] + sub_TVA_pars$S_rep[1:500,]
  
  ppc_dens_overlay(R, R_rep) +
    xlim(NA, quantile(R_rep, 0.99)) +
    theme(legend.position = "none") +
    labs(y = "density", x = "y_est", title = paste(i, "posterior predictive check"))
  
  p <- mcmc_combo(TVA.fits[[i]], pars = c("beta", "sigma_alpha", "sigma_R", "sigma_tot", "F_rw", 
                                          "mean_ln_R0"), 
                  combo = c("dens_overlay", "trace"),
                  gg_theme = legend_none()) |> 
    as.ggplot() +
    labs(title = paste(i, "leading parameters"))
    print(p)

  p <- mcmc_combo(TVA.fits[[i]], pars = c("D_scale", "D_sum"),
                  combo = c("dens_overlay", "trace"),
                  gg_theme = legend_none())|> 
    as.ggplot() +
    labs(title = paste(i, "age pars"))
    print(p)

  p <- mcmc_combo(TVA.fits[[i]], pars = c("Dir_alpha[1]", "Dir_alpha[2]", 
                                          "Dir_alpha[3]", "Dir_alpha[4]"),
                  combo = c("dens_overlay", "trace"),
                  gg_theme = legend_none())|> 
    as.ggplot() +
    labs(title = paste(i, "age probs"))
    print(p)
  cat('\n')
}
```

We hope minimum effective sample sizes are greater than ***2000***?

```{r ESS}
kable(min_ESS_AR1)
kable(min_ESS_TVA)
```

and that $\hat{R}$ are less than 1.05  

```{r rhat}
kable(max_Rhat_AR1)
kable(max_Rhat_TVA)
```

We see some problems with the time varying model, and have explored various parametrizations and options to include [semi-informative beta priors](https://github.com/Pacific-salmon-assess/yukon-CK-ResDoc/tree/main/analysis/Stan/semi_inform) that did not improve diagnostics. We imagine these issues in the time-varying model are due to the extreme, pronounced decline in productivity, and the correlation between productivity and the stationary capacity prior (i.e., Ricker $\beta$)